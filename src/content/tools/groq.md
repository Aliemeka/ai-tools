---
title: "Groq"
description: "Ultra-fast AI inference platform with blazing speed LLM API"
category: "development"
subcategory: "api-platform"
pricing: "freemium"
source: "closed-source"
website: "https://groq.com"
logo: "/images/tools/groq.svg"
tags: ["api", "inference", "speed", "llm", "fast", "performance"]
features:
  - "Lightning-fast LLM inference"
  - "Multiple model support (Llama, Mixtral, Gemma)"
  - "High-performance API endpoints"
  - "Low latency responses"
  - "Developer-friendly integration"
pricing_details:
  free: "14 requests per minute"
  paid: "Starting at $0.27/1M tokens"
api_available: true
mobile_app: false
integrations: ["REST API", "Python SDK", "Node.js", "cURL"]
last_updated: "2025-07-10"
rating: 4.5
user_count: "50K+"
---

## Overview

Groq is an AI inference platform that delivers exceptionally fast LLM responses through their custom hardware. It provides API access to various open-source models with industry-leading speed.

## Key Features

- **Ultra-Fast Inference**: Blazing fast response times for LLM requests
- **Multiple Models**: Support for Llama, Mixtral, Gemma, and other models
- **High Throughput**: Designed for high-volume applications
- **Low Latency**: Minimal delay between request and response
- **Developer Tools**: Comprehensive SDKs and documentation

## Use Cases

- Real-time chatbots and conversational AI
- High-performance AI applications
- Speed-critical AI inference
- Prototype and production AI systems
- API-driven AI integrations

## Pricing

- **Free Tier**: 14 requests per minute with rate limits
- **Pay-per-use**: $0.27 per 1M input tokens, $0.27 per 1M output tokens
- **Enterprise**: Custom pricing for high-volume usage
